# Real-Time Recommendation System

A production-grade, fault-tolerant ML system that ingests streaming user events and serves low-latency personalized recommendations.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React     â”‚â”€â”€â”€â”€â”€â–¶â”‚   Ingest     â”‚â”€â”€â”€â”€â”€â–¶â”‚  Kafka/Redpanda â”‚
â”‚  Frontend   â”‚      â”‚   Service    â”‚      â”‚   Event Stream  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Serve     â”‚â—€â”€â”€â”€â”€â”€â”‚   Feature    â”‚â—€â”€â”€â”€â”€â”€â”‚ Stream Processorâ”‚
â”‚  Service    â”‚      â”‚    Store     â”‚      â”‚   (Spark/Python)â”‚
â”‚ (FastAPI)   â”‚      â”‚  (Redis/DB)  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
       â”‚                    â”‚                      â”‚
       â”‚                    â–¼                      â–¼
       â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Vector DB   â”‚      â”‚   MongoDB       â”‚
                    â”‚(Pinecone/    â”‚      â”‚  Event Store    â”‚
                    â”‚   FAISS)     â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Trainer    â”‚
                    â”‚  (MLflow)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Airflow    â”‚
                    â”‚ Orchestrationâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

1. **Ingestion Service**: FastAPI service that receives user events and publishes to Kafka/Redpanda
2. **Stream Processor**: Spark Structured Streaming job (with Python fallback) that computes features in real-time
3. **Feature Store**: Redis for low-latency feature caching, PostgreSQL for persistent metadata
4. **Vector Store**: Pinecone (production) or FAISS (local dev) for similarity search
5. **Training Pipeline**: PyTorch/LightGBM models tracked with MLflow, orchestrated by Airflow
6. **Serving Service**: FastAPI endpoint that serves recommendations with caching and fallbacks
7. **Frontend**: React demo application

## ğŸš€ Quick Start (Local Development)

### Prerequisites

- Docker & Docker Compose
- Python 3.10+
- Node.js 18+ (for frontend)
- Make (optional, for convenience scripts)

### Step 1: Clone and Setup

```bash
git clone https://github.com/AyushBhure/recommendation-system.git
cd recommendation-system
cp .env.example .env
# Edit .env if needed (defaults work for local dev)
```

### Step 2: Start Infrastructure

```bash
# Start all services with Docker Compose
docker-compose -f infra/docker-compose.yml up -d

# Wait for services to be healthy (about 30 seconds)
docker-compose -f infra/docker-compose.yml ps
```

### Step 3: Initialize Databases

```bash
# Run database migrations and seed sample data
python scripts/bootstrap_sample_data.py
```

### Step 4: Start Services

In separate terminals:

```bash
# Terminal 1: Ingestion Service
cd services/ingest
python -m uvicorn main:app --reload --port 8000

# Terminal 2: Stream Processor
cd services/stream_processor
python main.py

# Terminal 3: Serving Service
cd services/serve
python -m uvicorn main:app --reload --port 8001

# Terminal 4: Frontend
cd frontend
npm install
npm start
```

### Step 5: Run Training (Optional)

```bash
cd services/trainer
python train.py --experiment-name recommendation-system
```

### Step 6: Test the System

```bash
# Run end-to-end smoke test
./scripts/run_smoke_test.sh
```

Or manually:

```bash
# Generate some events
curl -X POST http://localhost:8000/events \
  -H "Content-Type: application/json" \
  -d '{"user_id": "user1", "event_type": "view", "item_id": "item1", "timestamp": "2024-01-01T00:00:00Z"}'

# Wait a few seconds for processing, then get recommendations
curl "http://localhost:8001/recommend?user_id=user1&k=10"
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ infra/                    # Infrastructure as Code
â”‚   â”œâ”€â”€ docker-compose.yml    # Local dev environment
â”‚   â”œâ”€â”€ k8s/                  # Kubernetes manifests
â”‚   â””â”€â”€ terraform/            # Azure Terraform configs
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ingest/               # Event ingestion service
â”‚   â”œâ”€â”€ stream_processor/     # Spark/Python stream processor
â”‚   â”œâ”€â”€ trainer/              # Model training + MLflow
â”‚   â””â”€â”€ serve/                # Recommendation serving API
â”œâ”€â”€ frontend/                 # React demo application
â”œâ”€â”€ ops/                      # Operations scripts
â”‚   â”œâ”€â”€ prometheus/           # Prometheus config
â”‚   â”œâ”€â”€ grafana/              # Grafana dashboards
â”‚   â””â”€â”€ otlp/                 # OpenTelemetry config
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                 # Unit tests
â”‚   â””â”€â”€ e2e/                  # End-to-end tests
â”œâ”€â”€ scripts/                  # Utility scripts
â””â”€â”€ data/                     # Sample data (gitignored)
```

## ğŸ§ª Testing

### Unit Tests

```bash
# Run all unit tests
pytest tests/unit/ -v

# Run with coverage
pytest tests/unit/ --cov=services --cov-report=html
```

### End-to-End Tests

```bash
# Ensure Docker Compose is running
./scripts/run_smoke_test.sh
```

## ğŸ­ Production Deployment

### Kubernetes (AKS)

1. **Provision Azure Resources**:
   ```bash
   cd infra/terraform
   terraform init
   terraform plan
   terraform apply
   ```

2. **Deploy to AKS**:
   ```bash
   # Configure kubectl
   az aks get-credentials --resource-group recommendation-system-rg --name recommendation-aks

   # Apply manifests
   kubectl apply -f infra/k8s/
   ```

3. **Verify Deployment**:
   ```bash
   kubectl get pods -n recommendation-system
   kubectl get services -n recommendation-system
   ```

See `infra/k8s/README.md` for detailed deployment instructions.

## ğŸ”§ Configuration

All configuration is managed via environment variables. See `.env.example` for all available options.

Key configuration areas:
- **Databases**: PostgreSQL, MongoDB, Redis connection strings
- **Streaming**: Kafka/Redpanda brokers and topics
- **Vector Search**: Pinecone API key (or use FAISS locally)
- **MLflow**: Tracking URI and experiment name
- **Observability**: Prometheus, Grafana, OpenTelemetry endpoints

## ğŸ“Š Observability

### Metrics (Prometheus)

- Service: `http://localhost:9090`
- Metrics endpoint: `http://localhost:8001/metrics`

### Dashboards (Grafana)

- Service: `http://localhost:3001`
- Default credentials: `admin/admin`
- Pre-configured dashboards in `ops/grafana/dashboards/`

### Logs

Structured JSON logs are emitted by all services. View with:

```bash
docker-compose -f infra/docker-compose.yml logs -f [service-name]
```

## ğŸ“ Mentoring & Best Practices

See [MENTORING.md](./MENTORING.md) for:
- System design interview talking points
- Best practices for junior engineers
- Testing strategies
- Deployment patterns
- Observability practices

## ğŸ› ï¸ Development

### Code Quality

```bash
# Format code
black services/ tests/

# Lint
flake8 services/ tests/
pylint services/

# Type checking
mypy services/

# Pre-commit hooks (install with)
pre-commit install
```

### Adding New Features

1. Create feature branch
2. Write tests first (TDD)
3. Implement feature
4. Ensure all tests pass
5. Update documentation
6. Submit PR

## ğŸ” Security Notes

- **Never commit `.env` files** - use `.env.example` as template
- **Rotate secrets regularly** in production
- **Use Kubernetes secrets** for sensitive data
- **Enable TLS** for all database connections in production
- **Implement rate limiting** on public endpoints

## ğŸ“ Design Decisions & Tradeoffs

### At-Least-Once vs Exactly-Once

- **Current**: At-least-once delivery (simpler, lower latency)
- **Rationale**: Idempotency keys in events handle duplicates
- **Tradeoff**: Slight risk of duplicate processing (acceptable for recommendations)

### Redis vs PostgreSQL for Feature Cache

- **Redis**: Hot features, low-latency reads (<1ms)
- **PostgreSQL**: Persistent metadata, user/item profiles
- **Rationale**: Hybrid approach balances speed and durability

### Pinecone vs FAISS

- **Pinecone**: Production scale, managed service
- **FAISS**: Local dev, no external dependencies
- **Rationale**: Fallback ensures system works without cloud dependencies

### Spark vs Lightweight Python Consumer

- **Spark**: Production scale, complex aggregations
- **Python Consumer**: Local dev, simpler debugging
- **Rationale**: Both provided for flexibility

## ğŸ› Troubleshooting

### Services won't start

```bash
# Check Docker Compose logs
docker-compose -f infra/docker-compose.yml logs

# Verify ports aren't in use
netstat -an | grep -E '8000|8001|5432|6379|9092'
```

### Database connection errors

```bash
# Wait for databases to be ready
docker-compose -f infra/docker-compose.yml exec postgres pg_isready
docker-compose -f infra/docker-compose.yml exec mongodb mongosh --eval "db.adminCommand('ping')"
```

### No recommendations returned

1. Check if events are being ingested: `curl http://localhost:8000/health`
2. Verify stream processor is running and consuming events
3. Check Redis for cached features: `docker-compose exec redis redis-cli KEYS "*"`
4. Ensure model is trained and vectors are indexed

## ğŸ“š Additional Resources

- [Architecture Deep Dive](./docs/architecture.md)
- [API Documentation](./docs/api.md)
- [Deployment Guide](./infra/k8s/README.md)
- [Contributing Guidelines](./CONTRIBUTING.md)

## âœ… Checklist

See [CHECKLIST.md](./CHECKLIST.md) for a complete verification checklist.

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please read CONTRIBUTING.md first.

---

**Status**: âœ… Production-ready for local development. Azure deployment requires credentials setup.

